{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuning Custom Logistic Regression (GPU enabled if available)...\n",
      "Logistic Regression - Learning Rate: 0.001, Iterations: 1000, Accuracy: 97.58%\n",
      "Logistic Regression - Learning Rate: 0.001, Iterations: 2000, Accuracy: 98.30%\n",
      "Logistic Regression - Learning Rate: 0.001, Iterations: 5000, Accuracy: 98.74%\n",
      "Logistic Regression - Learning Rate: 0.01, Iterations: 1000, Accuracy: 98.83%\n",
      "Logistic Regression - Learning Rate: 0.01, Iterations: 2000, Accuracy: 98.74%\n",
      "Logistic Regression - Learning Rate: 0.01, Iterations: 5000, Accuracy: 98.57%\n",
      "Logistic Regression - Learning Rate: 0.1, Iterations: 1000, Accuracy: 98.65%\n",
      "Logistic Regression - Learning Rate: 0.1, Iterations: 2000, Accuracy: 98.74%\n",
      "Logistic Regression - Learning Rate: 0.1, Iterations: 5000, Accuracy: 98.48%\n",
      "Best Logistic Regression Parameters: {'learning_rate': 0.01, 'n_iters': 1000}, Accuracy: 98.83%\n",
      "\n",
      "Tuning Custom Naive Bayes...\n",
      "Naive Bayes - Alpha: 0.01, Accuracy: 98.57%\n",
      "Naive Bayes - Alpha: 0.1, Accuracy: 98.65%\n",
      "Naive Bayes - Alpha: 0.5, Accuracy: 98.65%\n",
      "Naive Bayes - Alpha: 1, Accuracy: 98.39%\n",
      "Naive Bayes - Alpha: 5, Accuracy: 92.91%\n",
      "Best Naive Bayes Parameters: {'alpha': 0.1}, Accuracy: 98.65%\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Load dataset\n",
    "file_path = r'C:\\\\Users\\\\anand\\\\Downloads\\\\minorprjct\\\\Data_set\\\\mail_data.csv'\n",
    "mail_data = pd.read_csv(file_path)\n",
    "\n",
    "# Check for missing values and convert labels to binary\n",
    "mail_data['Category'] = mail_data['Category'].map({'ham': 0, 'spam': 1})\n",
    "\n",
    "# Split data into features and target\n",
    "X = mail_data['Message']\n",
    "y = mail_data['Category']\n",
    "\n",
    "# Split into training and testing sets (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Vectorize the text data using TF-IDF\n",
    "vectorizer = TfidfVectorizer(stop_words='english', max_features=3000)\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = vectorizer.transform(X_test)\n",
    "\n",
    "# Function to preprocess sparse data using StandardScaler (for Logistic Regression)\n",
    "def preprocess_data(data):\n",
    "    scaler = StandardScaler(with_mean=False)  # Avoid dense conversion for sparse data\n",
    "    return scaler.fit_transform(data)\n",
    "\n",
    "# Scaling the TF-IDF data (for Logistic Regression)\n",
    "X_train_scaled = preprocess_data(X_train_tfidf)\n",
    "X_test_scaled = preprocess_data(X_test_tfidf)\n",
    "\n",
    "# --- Custom Logistic Regression using PyTorch ---\n",
    "class LogisticRegressionModel:\n",
    "    def __init__(self, learning_rate=0.01, n_iters=1000):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.n_iters = n_iters\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        X = torch.tensor(X.toarray(), dtype=torch.float32).to(self.device)\n",
    "        y = torch.tensor(y.values, dtype=torch.float32).to(self.device)\n",
    "        n_samples, n_features = X.shape\n",
    "        \n",
    "        # Initialize weights and bias on the correct device\n",
    "        self.weights = torch.zeros(n_features, dtype=torch.float32, device=self.device, requires_grad=True)\n",
    "        self.bias = torch.zeros(1, dtype=torch.float32, device=self.device, requires_grad=True)\n",
    "\n",
    "        # Optimizer and loss function\n",
    "        optimizer = optim.SGD([self.weights, self.bias], lr=self.learning_rate)\n",
    "        criterion = nn.BCELoss()  # Binary Cross-Entropy loss\n",
    "\n",
    "        # Training loop\n",
    "        for _ in range(self.n_iters):\n",
    "            linear_model = torch.matmul(X, self.weights) + self.bias\n",
    "            predictions = torch.sigmoid(linear_model)\n",
    "            loss = criterion(predictions, y)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    def predict(self, X):\n",
    "        X = torch.tensor(X.toarray(), dtype=torch.float32).to(self.device)\n",
    "        linear_model = torch.matmul(X, self.weights) + self.bias\n",
    "        predictions = torch.sigmoid(linear_model)\n",
    "        return [1 if i > 0.5 else 0 for i in predictions.cpu().detach().numpy()]\n",
    "\n",
    "# --- Custom Naive Bayes ---\n",
    "class NaiveBayesModel:\n",
    "    def __init__(self, alpha=1.0):\n",
    "        self.alpha = alpha\n",
    "        self.class_log_prior_ = None\n",
    "        self.feature_log_prob_ = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        n_samples, n_features = X.shape\n",
    "        self.classes = np.unique(y)\n",
    "        n_classes = len(self.classes)\n",
    "        \n",
    "        # Initialize log-prior and log-likelihood probabilities\n",
    "        self.class_log_prior_ = np.zeros(n_classes)\n",
    "        self.feature_log_prob_ = np.zeros((n_classes, n_features))\n",
    "        \n",
    "        # Loop over each class and compute the likelihood and prior\n",
    "        for idx, c in enumerate(self.classes):\n",
    "            X_c = X[y == c]\n",
    "            self.class_log_prior_[idx] = np.log(X_c.shape[0] / n_samples)\n",
    "            total_word_count = X_c.sum(axis=0) + self.alpha\n",
    "            total_class_word_count = total_word_count.sum()\n",
    "            self.feature_log_prob_[idx, :] = np.log(total_word_count / total_class_word_count)\n",
    "\n",
    "    def predict(self, X):\n",
    "        log_probs = (X @ self.feature_log_prob_.T) + self.class_log_prior_\n",
    "        return np.argmax(log_probs, axis=1)\n",
    "\n",
    "# --- Hyperparameter Tuning for Custom Logistic Regression ---\n",
    "log_reg_grid = {\n",
    "    'learning_rate': [0.001, 0.01, 0.1],\n",
    "    'n_iters': [1000, 2000, 5000]\n",
    "}\n",
    "\n",
    "def tune_logistic_regression(X_train, y_train, X_test, y_test):\n",
    "    best_acc = 0\n",
    "    best_params = {}\n",
    "    for lr in log_reg_grid['learning_rate']:\n",
    "        for n_iter in log_reg_grid['n_iters']:\n",
    "            model = LogisticRegressionModel(learning_rate=lr, n_iters=n_iter)\n",
    "            model.fit(X_train, y_train)\n",
    "            y_pred = model.predict(X_test)\n",
    "            accuracy = accuracy_score(y_test, y_pred)\n",
    "            print(f\"Logistic Regression - Learning Rate: {lr}, Iterations: {n_iter}, Accuracy: {accuracy * 100:.2f}%\")\n",
    "            if accuracy > best_acc:\n",
    "                best_acc = accuracy\n",
    "                best_params = {'learning_rate': lr, 'n_iters': n_iter}\n",
    "    print(f\"Best Logistic Regression Parameters: {best_params}, Accuracy: {best_acc * 100:.2f}%\")\n",
    "    return best_params\n",
    "\n",
    "# --- Hyperparameter Tuning for Custom Naive Bayes ---\n",
    "nb_grid = {'alpha': [0.01, 0.1, 0.5, 1, 5]}\n",
    "\n",
    "def tune_naive_bayes(X_train, y_train, X_test, y_test):\n",
    "    best_acc = 0\n",
    "    best_params = {}\n",
    "    for alpha in nb_grid['alpha']:\n",
    "        model = NaiveBayesModel(alpha=alpha)\n",
    "        model.fit(X_train.toarray(), y_train)\n",
    "        y_pred = model.predict(X_test.toarray())\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        print(f\"Naive Bayes - Alpha: {alpha}, Accuracy: {accuracy * 100:.2f}%\")\n",
    "        if accuracy > best_acc:\n",
    "            best_acc = accuracy\n",
    "            best_params = {'alpha': alpha}\n",
    "    print(f\"Best Naive Bayes Parameters: {best_params}, Accuracy: {best_acc * 100:.2f}%\")\n",
    "    return best_params\n",
    "\n",
    "# --- Main Execution ---\n",
    "# Tune Logistic Regression\n",
    "print(\"Tuning Custom Logistic Regression (GPU enabled if available)...\")\n",
    "best_log_reg_params = tune_logistic_regression(X_train_scaled, y_train, X_test_scaled, y_test)\n",
    "\n",
    "# Tune Naive Bayes\n",
    "print(\"\\nTuning Custom Naive Bayes...\")\n",
    "best_nb_params = tune_naive_bayes(X_train_tfidf, y_train, X_test_tfidf, y_test)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
